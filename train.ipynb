{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [2.444162]\n",
      "epoch: 0, batch: 200, loss is: [1.8300562]\n",
      "epoch: 0, batch: 400, loss is: [1.3318727]\n",
      "epoch: 0, batch: 600, loss is: [1.0272088]\n",
      "epoch: 0, batch: 800, loss is: [0.57753646]\n",
      "epoch: 0, batch: 1000, loss is: [0.94898736]\n",
      "epoch: 0, batch: 1200, loss is: [0.55481696]\n",
      "epoch: 0, batch: 1400, loss is: [0.44957757]\n",
      "epoch: 0, batch: 1600, loss is: [0.24114574]\n",
      "epoch: 0, batch: 1800, loss is: [0.2672563]\n",
      "epoch: 0, batch: 2000, loss is: [0.53473794]\n",
      "epoch: 0, batch: 2200, loss is: [0.29776996]\n",
      "epoch: 0, batch: 2400, loss is: [0.18626039]\n",
      "epoch: 0, batch: 2600, loss is: [0.06019416]\n",
      "epoch: 0, batch: 2800, loss is: [0.19380222]\n",
      "epoch: 0, batch: 3000, loss is: [0.17110974]\n",
      "epoch: 0, batch: 3200, loss is: [0.25835514]\n",
      "epoch: 0, batch: 3400, loss is: [0.20399052]\n",
      "epoch: 0, batch: 3600, loss is: [0.5912686]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 154\u001B[0m\n\u001B[0;32m    151\u001B[0m test_preds \u001B[38;5;241m=\u001B[39m paddle\u001B[38;5;241m.\u001B[39mconcat(class_preds)\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m--> 154\u001B[0m     \u001B[43mlogwriter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_pr_curve\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mclass_\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtest_preds\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_probs\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_thresholds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    156\u001B[0m avg_acc, avg_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(accuracies), np\u001B[38;5;241m.\u001B[39mmean(losses)\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[validation]After epoch \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m: accuracy/loss: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(epoch_id, avg_acc, avg_loss))\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2023.1\\projects\\workspace\\venv\\lib\\site-packages\\visualdl\\writer\\writer.py:574\u001B[0m, in \u001B[0;36mLogWriter.add_pr_curve\u001B[1;34m(self, tag, labels, predictions, step, num_thresholds, weights, walltime)\u001B[0m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m% c\u001B[39;00m\u001B[38;5;124man\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt appear in tag!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    572\u001B[0m walltime \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mround\u001B[39m(time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m walltime \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m walltime\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_file_writer()\u001B[38;5;241m.\u001B[39madd_record(\n\u001B[1;32m--> 574\u001B[0m     \u001B[43mpr_curve\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    575\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtag\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    576\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    577\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpredictions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpredictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    578\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    579\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwalltime\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwalltime\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    580\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_thresholds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_thresholds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    581\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2023.1\\projects\\workspace\\venv\\lib\\site-packages\\visualdl\\component\\base_component.py:440\u001B[0m, in \u001B[0;36mpr_curve\u001B[1;34m(tag, labels, predictions, step, walltime, num_thresholds, weights)\u001B[0m\n\u001B[0;32m    424\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Package data to one pr_curve.\u001B[39;00m\n\u001B[0;32m    425\u001B[0m \n\u001B[0;32m    426\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    437\u001B[0m \u001B[38;5;124;03m    Package with format of record_pb2.Record\u001B[39;00m\n\u001B[0;32m    438\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    439\u001B[0m num_thresholds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(num_thresholds, \u001B[38;5;241m127\u001B[39m)\n\u001B[1;32m--> 440\u001B[0m prcurve_map \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_curve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_thresholds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pr_curve_raw(\n\u001B[0;32m    443\u001B[0m     tag\u001B[38;5;241m=\u001B[39mtag,\n\u001B[0;32m    444\u001B[0m     tp\u001B[38;5;241m=\u001B[39mprcurve_map[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtp\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    450\u001B[0m     step\u001B[38;5;241m=\u001B[39mstep,\n\u001B[0;32m    451\u001B[0m     walltime\u001B[38;5;241m=\u001B[39mwalltime)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2023.1\\projects\\workspace\\venv\\lib\\site-packages\\visualdl\\component\\base_component.py:386\u001B[0m, in \u001B[0;36mcompute_curve\u001B[1;34m(labels, predictions, num_thresholds, weights)\u001B[0m\n\u001B[0;32m    383\u001B[0m     weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m\n\u001B[0;32m    385\u001B[0m bucket_indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mint32(np\u001B[38;5;241m.\u001B[39mfloor(predictions \u001B[38;5;241m*\u001B[39m (num_thresholds \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)))\n\u001B[1;32m--> 386\u001B[0m float_labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mastype(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m)\n\u001B[0;32m    387\u001B[0m histogram_range \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m0\u001B[39m, num_thresholds \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    388\u001B[0m tp_buckets, _ \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mhistogram(\n\u001B[0;32m    389\u001B[0m     bucket_indices,\n\u001B[0;32m    390\u001B[0m     bins\u001B[38;5;241m=\u001B[39mnum_thresholds,\n\u001B[0;32m    391\u001B[0m     \u001B[38;5;28mrange\u001B[39m\u001B[38;5;241m=\u001B[39mhistogram_range,\n\u001B[0;32m    392\u001B[0m     weights\u001B[38;5;241m=\u001B[39mfloat_labels \u001B[38;5;241m*\u001B[39m weights)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2023.1\\projects\\workspace\\venv\\lib\\site-packages\\numpy\\__init__.py:305\u001B[0m, in \u001B[0;36m__getattr__\u001B[1;34m(attr)\u001B[0m\n\u001B[0;32m    300\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    301\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn the future `np.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` will be defined as the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcorresponding NumPy scalar.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;129;01min\u001B[39;00m __former_attrs__:\n\u001B[1;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(__former_attrs__[attr])\n\u001B[0;32m    307\u001B[0m \u001B[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001B[39;00m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001B[39;00m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001B[39;00m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001B[39;00m\n\u001B[0;32m    312\u001B[0m \u001B[38;5;66;03m# the full `numpy.testing` namespace\u001B[39;00m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtesting\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "# 加载飞桨相关库\n",
    "import paddle\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "import paddle.nn.functional as F\n",
    "# 加载visualdl\n",
    "from visualdl import LogWriter\n",
    "\n",
    "logwriter = LogWriter(logdir='./runs/mnist_experiment')\n",
    "\n",
    "# 数据载入\n",
    "class MNISTDataset():\n",
    "    def __init__(self, mode='train'):\n",
    "        self.mnist_data = paddle.vision.datasets.MNIST(mode=mode)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.mnist_data[idx]\n",
    "        data = np.reshape(data, [1, 28, 28]).astype('float32') / 255\n",
    "        label = np.reshape(label, [1]).astype('int64')\n",
    "        return (data, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_data)\n",
    "\n",
    "# 查看9张输入的训练图像的样例\n",
    "# dataset = MNISTDataset(mode='train')\n",
    "# image_matrix = []\n",
    "# for i in range(9):\n",
    "#     image, label = dataset[i]\n",
    "#     # 将dataset中的CHW排列的图像转换成HWC排列再写入VisualDL\n",
    "#     image_matrix.append(image.transpose([1,2,0]))\n",
    "\n",
    "\n",
    "# 将九张输入图像合成长宽相同的图像网格，即3X3的图像网格\n",
    "# logwriter.add_image_matrix(tag='input_images', step=1, imgs=image_matrix, rows=-1)\n",
    "# tags = ['image_{}'.format(i) for i in range(9)]\n",
    "# logwriter.add_embeddings('input_image_embeddings', mat=[img.reshape(-1) for img in image_matrix], metadata=tags)\n",
    "\n",
    "train_loader = paddle.io.DataLoader(MNISTDataset(mode='train'),\n",
    "                                    batch_size=16,\n",
    "                                    shuffle=True)\n",
    "\n",
    "test_loader = paddle.io.DataLoader(MNISTDataset(mode='test'),\n",
    "                                   batch_size=16,\n",
    "                                   shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 定义mnist数据识别网络模型结构\n",
    "class MNIST(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MNIST, self).__init__()\n",
    "\n",
    "        # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n",
    "        self.conv1 = Conv2D(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义池化层，池化核的大小kernel_size为2，池化步长为2\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n",
    "        self.conv2 = Conv2D(in_channels=20, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义池化层，池化核的大小kernel_size为2，池化步长为2\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义一层全连接层，输出维度是10\n",
    "        self.fc = Linear(in_features=980, out_features=10)\n",
    "\n",
    "    # 定义网络前向计算过程，卷积后紧接着使用池化层，最后使用全连接层计算最终输出\n",
    "    # 卷积层激活函数使用Relu，全连接层激活函数使用softmax\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "#创建模型\n",
    "model = MNIST()\n",
    "\n",
    "#保存动态图模型为静态图\n",
    "paddle.jit.save(model, './runs/mnist_experiment/model', [paddle.static.InputSpec([-1,1,28,28])])\n",
    "\n",
    "#设置优化器\n",
    "opt = paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters())\n",
    "EPOCH_NUM = 10\n",
    "\n",
    "logwriter.add_hparams(hparams_dict={'lr': 0.001, 'batch_size': 16, 'opt': 'sgd'},\n",
    "                      metrics_list=['train_avg_loss', 'test_avg_loss', 'test_avg_acc'])\n",
    "\n",
    "for epoch_id in range(EPOCH_NUM):\n",
    "    model.train()\n",
    "    train_batchs_per_epoch = len(train_loader)\n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        #准备数据\n",
    "        images, labels = data\n",
    "\n",
    "        #前向计算的过程\n",
    "        predicts = model(images)\n",
    "\n",
    "        #计算损失，取一个批次样本损失的平均值\n",
    "        loss = F.cross_entropy(predicts, labels)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "\n",
    "        #记录当前训练Loss到VisualDL\n",
    "        logwriter.add_scalar(\"train_avg_loss\", value=avg_loss.numpy(), step=batch_id+epoch_id*(train_batchs_per_epoch))\n",
    "\n",
    "        #记录网络中最后一个fc层的参数到VisualDL\n",
    "        logwriter.add_histogram(\"fc_weight\", values=model.fc.weight.numpy(), step=batch_id+epoch_id*(train_batchs_per_epoch))\n",
    "\n",
    "        #每训练了100批次的数据，打印下当前Loss的情况\n",
    "        if batch_id % 200 == 0:\n",
    "            print(\"epoch: {}, batch: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\n",
    "\n",
    "        #后向传播，更新参数的过程\n",
    "        avg_loss.backward()\n",
    "        # 最小化loss,更新参数\n",
    "        opt.step()\n",
    "        # 清除梯度\n",
    "        opt.clear_grad()\n",
    "\n",
    "    # evaluate model after one epoch\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    class_probs = []\n",
    "    class_preds = []\n",
    "    for batch_id, data in enumerate(test_loader):\n",
    "        #准备数据\n",
    "        images, labels = data\n",
    "        #前向计算的过程\n",
    "        predicts = model(images)\n",
    "        #计算损失\n",
    "        loss = F.cross_entropy(predicts, labels)\n",
    "        #计算准确率\n",
    "        acc = paddle.metric.accuracy(predicts, labels)\n",
    "        accuracies.append(acc.numpy())\n",
    "        losses.append(loss.numpy())\n",
    "        #记录用于画pr曲线需要的预测概率和类别\n",
    "        class_probs_batch = [F.softmax(predict, axis=0) for predict in predicts]\n",
    "        class_preds_batch = paddle.argmax(predicts, 1)\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_preds.append(class_preds_batch)\n",
    "\n",
    "    test_probs = paddle.concat([paddle.stack(batch) for batch in class_probs]).numpy()\n",
    "    test_preds = paddle.concat(class_preds).numpy()\n",
    "\n",
    "    for i in range(10):\n",
    "        logwriter.add_pr_curve('class_{}'.format(i), labels=(test_preds == i),predictions=test_probs[:,i], num_thresholds=100, step=epoch_id)\n",
    "\n",
    "    avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "    print(\"[validation]After epoch {}: accuracy/loss: {}/{}\".format(epoch_id, avg_acc, avg_loss))\n",
    "    #记录当前测试集平均Loss和准确率到VisualDL\n",
    "    logwriter.add_scalar(\"test_avg_loss\", value=avg_loss, step=epoch_id)\n",
    "    logwriter.add_scalar(\"test_avg_acc\", value=avg_acc, step=epoch_id)\n",
    "\n",
    "#保存模型参数\n",
    "paddle.save(model.state_dict(), 'mnist.pdparams')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T04:32:52.530217300Z",
     "start_time": "2023-06-13T04:30:43.507927700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
